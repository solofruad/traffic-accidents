{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.2 Output Datasets \n",
    "### By **Néstor Suat** in 2019\n",
    "\n",
    "**Descripción:** Generando las dos propuestas de conjunto de datos para entrenar el word embedding **(Doc2vec)** \n",
    "\n",
    "**Input:**\n",
    "Conjunto de datos recolectados utilizando Twitter API\n",
    "* Twitter Search API con keywords y coordenadas de Bogotá\n",
    "* Twitter Search API de Timeline de Users relevantes\n",
    "* Twitter Stream API de Timeline de Users relevantes, también menciones\n",
    "* Twitter Stream API con coordenadas de Bogotá\n",
    "\n",
    "**Output:**\n",
    "* 50% bogota + 50% no_bogota = **dataset_propuesta1_5050**\n",
    "* Complete, all tweets =  **dataset_propuesta2_complete**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando Conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_api = pd.read_csv(\"../../../data/database/server_token_search.tsv\", delimiter = \"\\t\", quoting = 3)\n",
    "user_search_api = pd.read_csv(\"../../../data/database/server_token_user.tsv\", delimiter = \"\\t\", quoting = 3)\n",
    "timeline_stream_api = pd.read_csv(\"../../../data/database/server_follow_timeline_user.tsv\", delimiter = \"\\t\", quoting = 3)\n",
    "bogota_stream_api.to_csv(\"../../../data/v1/doc2vec/bogota_stream_api.tsv\",sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se toma todos los tweets anteriores, eliminando los tweets duplicados. Primero realiza esta unión exceptuando los tweets recolectados con **coordendas de Bogotá** usando **Stream API Twitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_no_bogota = pd.concat([search_api, user_search_api, timeline_stream_api])\n",
    "dataset_no_bogota = dataset_no_bogota[[\"text\",\"id_tweet\"]]\n",
    "dataset_no_bogota = dataset_no_bogota.drop_duplicates(['id_tweet'],keep='first')\n",
    "dataset_no_bogota = dataset_no_bogota.drop_duplicates(['text'],keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se exporta el conjunto de datos en un archivo para usarlo más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_no_bogota = dataset_no_bogota[[\"text\"]]\n",
    "dataset_no_bogota.to_csv(\"data/v1/doc2vec/no_bogota.tsv\",sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segundo importa los tweets recolectados en tiempo real de Bogotá, elimina los tweets duplicados. Estos tweets recolectados brindan versatilidad en el lenguaje empleado, no son tweets filtrados, por lo que se puede encontrar información publicada popularmente en los usuarios de esta red social, estos textos son útiles para entrenar un modelo de embedding que tenga en cuenta diferentes formas de contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bogota_stream_api = pd.read_csv(\"data/database/server_bogota.tsv\", delimiter = \"\\t\", quoting = 3)\n",
    "bogota_stream_api = bogota_stream_api[[\"text\",\"id_tweet\"]]\n",
    "bogota_stream_api = bogota_stream_api.drop_duplicates(['id_tweet'],keep='first')\n",
    "bogota_stream_api = bogota_stream_api.drop_duplicates(['text'],keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Generando la **propuesta 1**\n",
    "La propuesta 1 genera un dataset que contiene 50% tweets de Stream API Bogotá y 50% utilizando los otros métodos, así se asegura que el modelo de embedding tenga una muestra equilibrada de textos populares en twitter, como los textos relacionados a tránsito o de usuarios relevantes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bogota_stream_api = bogota_stream_api.sample(frac=1)\n",
    "#n=572510 representa el 50% de la cantidad final del conjunto de datos propuesta 1\n",
    "bogota_stream_api = bogota_stream_api.sample(n=572510)\n",
    "bogota_stream_api = bogota_stream_api[[\"text\"]]\n",
    "\n",
    "#bogota_stream_api.to_csv(\"data/v1/doc2vec/bogota_stream_api.tsv\",sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_propuesta1 = pd.concat([bogota_stream_api, dataset_no_bogota])\n",
    "dataset_propuesta1 = dataset_propuesta1.sample(frac=1)\n",
    "\n",
    "dataset_propuesta1.to_csv(\"data/v1/doc2vec/dataset_propuesta1.tsv\",sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generando la **propuesta 2**\n",
    "Se utiliza el conjunto de datos exportados anteriormente de la unión de las otras bases de datos. Se toma este conjunto y se añade al conjunto de datos recolectados con **Stream API** con las coordenadas de Bogotá."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_bogota = pd.read_csv(\"data/v1/doc2vec/no_bogota.tsv\", delimiter = \"\\t\", quoting = 3)\n",
    "del no_bogota['Unnamed: 0']\n",
    "\n",
    "dataset_propuesta2_complete = pd.concat([no_bogota, bogota_stream_api])\n",
    "dataset_propuesta2_complete = dataset_propuesta2_complete.sample(frac=1)\n",
    "dataset_propuesta2_complete.to_csv(\"data/v1/doc2vec/dataset_propuesta2_complete.tsv\",sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
