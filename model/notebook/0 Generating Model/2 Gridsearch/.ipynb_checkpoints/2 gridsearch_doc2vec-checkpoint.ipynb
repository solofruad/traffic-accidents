{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridsearch hyperparameters SVM con Doc2vec\n",
    "### By **N√©stor Suat** in 2019\n",
    "\n",
    "**Descripci√≥n:** Buscando los parametros adecuados para el modelo **SVM** usando **Doc2vec** como embedding. \n",
    "\n",
    "**Input:**\n",
    "* Train and Test set\n",
    "* Doc2vec model (DBOW or DMM or both of them)\n",
    "* Hyperparameters\n",
    "\n",
    "**Output:**\n",
    "* The best model with parameters\n",
    "* Metrics: confusion matrix, accuracy, recall, precision and F1-score\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Cargando datos y limpieza\n",
    "\n",
    "### Importando librer√≠as\n",
    "\n",
    "Como estamos en un archivo afuera se necesita agregar la direcci√≥n ../ (ra√≠z del proyexto) para importar la librer√≠a de preprocesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, '../../../')\n",
    "\n",
    "from classes.doc2vec.preprocessing import Preprocessing as doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3804, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>üì¢#Atenci√≥n: se presenta siniestro vial entre u...</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>üì¢#Atenci√≥n: a esta hora se presentan disturbio...</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Incidente vial entre taxi üöñ y‚Äç motocicleta üèçÔ∏è ...</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@chemabernal @Moniva0517 @MartinSantosR La gr√°...</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @CaracolRadio: #CaracolEsM√°s | ¬°Atenci√≥n! F...</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  dataset\n",
       "0  üì¢#Atenci√≥n: se presenta siniestro vial entre u...      1       99\n",
       "1  üì¢#Atenci√≥n: a esta hora se presentan disturbio...      0       99\n",
       "2  Incidente vial entre taxi üöñ y‚Äç motocicleta üèçÔ∏è ...      1       99\n",
       "3  @chemabernal @Moniva0517 @MartinSantosR La gr√°...      0       99\n",
       "4  RT @CaracolRadio: #CaracolEsM√°s | ¬°Atenci√≥n! F...      1       99"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"../../../data/v1/7030/train70.tsv\", delimiter = \"\\t\", quoting = 3)\n",
    "train['dataset'] = 99 # train = 1\n",
    "test = pd.read_csv(\"../../../data/v1/7030/test30.tsv\", delimiter = \"\\t\", quoting = 3)\n",
    "test['dataset'] = 100 # test = 0\n",
    "dataset = pd.concat([train,test])\n",
    "dataset = dataset.reset_index(drop=True)\n",
    "print(dataset.shape) # (3804, 3)\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "#directory = \"../../../data/v1/doc2vec/\"\n",
    "directory = \"../../../data/v1/doc2vec/v2/\"\n",
    "file = \"5_clean_stem_dataset_propuesta1_5050\"\n",
    "type_clean = 5 #Tiene que ser el mismo que 'file' (prefijo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = doc2vec(dataset)\n",
    "clean.fit_clean(type_clean)\n",
    "\n",
    "embendding = clean.feature_extraction_dbow(directory, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test set\n",
    "Para el preprocesamiento uno los conjuntos, aqu√≠ vuelvo a separarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs_train = embendding[embendding[:,0] == 99.0,:] #train = 99\n",
    "vecs_test = embendding[embendding[:,0] == 100.0,:] #test = 100\n",
    "\n",
    "X_train = vecs_train[:,2:]\n",
    "y_train = vecs_train[:,1]\n",
    "X_test = vecs_test[:,2:]\n",
    "y_test = vecs_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size vecs_train (2662, 202)\n",
      "Size vecs_test (1142, 202)\n",
      "Size: \n",
      " * X_train: (2662, 200) \n",
      " * y_train: (2662,) \n",
      " * X_test: (1142, 200) \n",
      " * y_test: (1142,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size vecs_train\", vecs_train.shape)\n",
    "print(\"Size vecs_test\", vecs_test.shape)\n",
    "print(\"Size: \\n * X_train: %s \\n * y_train: %s \\n * X_test: %s \\n * y_test: %s\" % (X_train.shape, y_train.shape, X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Importando Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from pprint import pprint\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Random Search Training"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4, 7]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "random_grid"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 150, cv = 3, verbose=2, random_state=100, n_jobs = -1)# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Configurando el archivo donde se va guardar el resultado (info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging  # Setting up the loggings to monitor gensim\n",
    "\n",
    "logger = logging.getLogger(\"gridsearch\")\n",
    "hdlr = logging.FileHandler(\"gridsearch_doc2vec.log\")\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "hdlr.setFormatter(formatter)\n",
    "logger.addHandler(hdlr)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3. Comenzando a entrenar modelo**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "logger.info(\"#####Comenzando a entrenar modelo######\")    \n",
    "logger.info(__doc__)\n",
    "pipeline = Pipeline([      \n",
    "  ('clf', SVC(random_state=123) )\n",
    "])\n",
    "parameters = {          \n",
    "        'clf__kernel': ('linear', 'poly', 'rbf'),              \n",
    "        'clf__C': (0.01, 0.05, 0.1, 1, 2, 3, 4, 5, 6, 7, 8),\n",
    "        'clf__gamma': (0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.7, 1, 2, 3,10)            \n",
    "}\n",
    "\"\"\"\n",
    "parameters = {          \n",
    "        'clf__kernel': ('poly', 'rbf'),              \n",
    "        'clf__C': (5, 6),\n",
    "        'clf__gamma': (0.1, 0.2, 0.3, 0.4)            \n",
    "}\n",
    "\"\"\"\n",
    "scores = ['accuracy', 'f1']   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"#####Comenzando a entrenar modelo######\")    \n",
    "logger.info(__doc__)\n",
    "pipeline = Pipeline([      \n",
    "  ('clf', RandomForestClassifier(random_state=100,bootstrap=False, max_features='auto') )\n",
    "])\n",
    "\n",
    "\"\"\"parameters = {'clf__n_estimators': [500, 600, 800],\n",
    "               'clf__max_features': ['log2', 'auto'],\n",
    "               'clf__max_depth': [30, 40, 70, 100, None],\n",
    "               'clf__min_samples_split': [2, 5, 10],\n",
    "               'clf__min_samples_leaf': [1, 2, 4],\n",
    "               'clf__bootstrap': [True, False],\n",
    "             }  \"\"\"\n",
    "parameters = {'clf__n_estimators': [600,1000, 1200, 1600, 2000],                             \n",
    "               'clf__max_depth': [40, 50, 100, 110],\n",
    "               'clf__min_samples_split': [2, 4, 5, 10],\n",
    "               'clf__min_samples_leaf': [1, 2, 4],               \n",
    "             }    \n",
    "\n",
    "scores = ['accuracy', 'f1']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000],\n",
    " 'max_features': ['auto', 'sqrt', 'log2'],\n",
    " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
    " 'min_samples_split': [2, 5, 10],\n",
    " 'min_samples_leaf': [1, 2, 4, 7],\n",
    " 'bootstrap': [True, False]}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "try:\n",
    "    for score in scores:\n",
    "        logger.info(\"# Tuning hyper-parameters for %s\" % score)\n",
    "        logger.info(\" \")\n",
    "    \n",
    "        logger.info(\"Performing grid search...\")\n",
    "        print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "        logger.info(\"parameters:\")\n",
    "        pprint(parameters)\n",
    "        t0 = time()\n",
    "        grid_search = GridSearchCV(pipeline, parameters, cv=5, scoring=score, n_jobs=-1,verbose=1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        logger.info(\"done in %0.3fs\" % (time() - t0))\n",
    "        logger.info(\" \")\n",
    "        \n",
    "        logger.info(\"Best parameters set found on development set:\")\n",
    "        logger.info(\" \")\n",
    "        logger.info(grid_search.best_params_)\n",
    "        logger.info(\" \")\n",
    "        ##Old start\n",
    "        logger.info(\"--\")\n",
    "        logger.info(\"Best score: %0.3f\" % grid_search.best_score_)    \n",
    "        logger.info(\"Best parameters set:\")\n",
    "        best_parameters = grid_search.best_estimator_.get_params()    \n",
    "        for param_name in sorted(parameters.keys()):\n",
    "            logger.info(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        logger.info(\"--\")\n",
    "        logger.info(\" \")\n",
    "        \n",
    "        logger.info(\"Grid scores on development set:\")\n",
    "        logger.info(\" \")\n",
    "        means = grid_search.cv_results_['mean_test_score']\n",
    "        stds = grid_search.cv_results_['std_test_score']\n",
    "        for mean, std, params in sorted(zip(means, stds, grid_search.cv_results_['params']),key = lambda t: t[0],reverse=True):\n",
    "            logger.info(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "        logger.info(\" \")\n",
    "    \n",
    "        logger.info(\"Detailed classification report:\")\n",
    "        logger.info(\" \")\n",
    "        logger.info(\"The model is trained on the full development set.\")\n",
    "        logger.info(\"The scores are computed on the full evaluation set.\")\n",
    "        logger.info(\" \")\n",
    "        y_true, y_pred = y_test, grid_search.predict(X_test)\n",
    "        logger.info(classification_report(y_true, y_pred))\n",
    "        logger.info(\" \")\n",
    "        \n",
    "except Exception as e:\n",
    "    logger.error('Unhandled exception:')\n",
    "    logger.error(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
