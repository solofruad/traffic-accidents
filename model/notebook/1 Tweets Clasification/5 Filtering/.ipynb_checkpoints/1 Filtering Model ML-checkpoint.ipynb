{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering Model ML\n",
    "### By **Néstor Suat** in 2020\n",
    "\n",
    "**Descripción:** Proceso de filtrado de tweet recolectados seleccionando un modelo de clasificación automática generado en los pasos anteriores o pipeline.\n",
    "\n",
    "**Input:**\n",
    "* Model saved in pickel (.pkl)\n",
    "* Dataset Tweet\n",
    "* Model selected\n",
    "\n",
    "**Output:**\n",
    "* Dataset Tweet accident\n",
    "* Dataset Tweet no accident\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando librerías\n",
    "\n",
    "Como estamos en un archivo afuera se necesita agregar la dirección ../ (raíz del proyexto) para importar la librería de preprocesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../../')\n",
    "\n",
    "from classes.tweet2accident.preprocessing import Preprocessing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1_server_bogota.tsv -- 2,9 GB -- # 4’027.313 Tweets\n",
    "2_server_token_search.tsv -- 119.4 MB -- # 271.153 Tweets\n",
    "3_server_token_user.tsv -- 43.8 MB -- # 100.618 Tweets\n",
    "4_server_follow_timeline_user.tsv -- 246.2 MB -- # 574.816 Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"1_server_bogota.tsv\"\n",
    "#M1\n",
    "filemodel = 'model_m1_dbow_svm.pkl'\n",
    "dir_ = \"../../../data/database/output_ml/M1/\"\n",
    "\n",
    "#M2\n",
    "#filemodel = 'model_m2_tfidf_svm.pkl'\n",
    "#dir_ = \"../../../data/database/output_ml/M2/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = pd.read_csv(\"../../../data/database/server_bogota/\"+file, delimiter = \"\\t\", quoting = 3)\n",
    "dataset = pd.read_csv(\"../../../data/database/\"+file, delimiter = \"\\t\", quoting = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4027313, 24)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando modelo de clasificación automatica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open(filemodel, 'rb')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proceso de filtrado de tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_predict = model.predict(dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de total (seg.)\n",
      "13431.809956961\n"
     ]
    }
   ],
   "source": [
    "end = time.perf_counter()\n",
    "print(\"Tiempo de total (seg.)\")\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['label'] = text_predict\n",
    "#dataset['label_3'] = text_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "accident = dataset[dataset['label'] == 1 ]\n",
    "no_accident = dataset[dataset['label'] == 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(dir_+\"clf_\"+file,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accident dataset Size:(5832, 25)\n",
      "No Accident dataset Size:(4021481, 25)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accident dataset Size:\"+str(accident.shape))\n",
    "print(\"No Accident dataset Size:\"+str(no_accident.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_output = file\n",
    "accident.to_csv(dir_+\"accident_\"+name_output,sep='\\t')\n",
    "no_accident.to_csv(dir_+\"no_accident_\"+name_output,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tweet_by_username = accident['user_name'].value_counts() # Show distribution of tweets by user\n",
    "count_tweet_by_username[:10]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "accident.iloc[55]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "for i in range(len(accident)):    \n",
    "    print(\"- \"+accident.iloc[i]['text'],sep='\\n')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "accident_clasification_model.predict([\"Dos canastas.\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic",
   "language": "python",
   "name": "traffic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
