{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Dataset. Part 1\n",
    "### By **Néstor Suat** in 2020\n",
    "\n",
    "**Descripción:** Contruyendo el dataset Parte 1. Generar los archivos para BRAT Annotation Tools.\n",
    "\n",
    "**Input:**\n",
    "* TSV con los tweets seleccionados\n",
    "\n",
    "**Output:**\n",
    "* Files *.txt and *.ann\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando librerías\n",
    "\n",
    "Como estamos en un archivo afuera se necesita agregar la dirección ../ (raíz del proyexto) para importar la librería de preprocesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy  # For preprocessing\n",
    "import re\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../../')\n",
    "\n",
    "from classes.wordsegmentation import WordSegmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"es_core_news_lg\") # disabling Named Entity Recognition for speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'ner_dataset_shuffle.tsv'\n",
    "dir_ = \"../../../data/v1/NER/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(dir_+file, delimiter = \"\\t\", quoting = 3)\n",
    "del dataset['Unnamed: 0']\n",
    "dataset = dataset['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seleccionando 50\n",
    "df = dataset[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['45, ', 'norte']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmentation.segment(\"45, norte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicando limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg = WordSegmentation(dir_+'spanish_count_1w_small_v2_twitter.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_fn(doc):\n",
    "    pre = []\n",
    "    for token in doc.text.split(' '):\n",
    "        if len(token) > 0:\n",
    "            if token[0] == '@' or token[0] == '#':\n",
    "                token = ' '.join(seg.segment(token[1:]))\n",
    "            pre.append(token)    \n",
    "    return ' '.join(pre)\n",
    "                \n",
    "    \n",
    "def preText(text):    \n",
    "    #pre = re.sub(\"(@[A-Za-z0-9_]+)\", '[MASK]', text) #Reemplazar @username por [UNK]    \n",
    "    pre = re.sub(\"&[A-Za-z]+;\", ' ', text) #Eliminar códigos ASCII\n",
    "    pre = re.sub(\"(\\w+:\\/\\/\\S+)\",' ',pre) #Eliminar links http y https\n",
    "    pre = re.sub(\"([^A-Za-z0-9äÄëËïÏöÖüÜáéíóúáéíóúÁÉÍÓÚÂÊÎÔÛâêîôûàèìòùÀÈÌÒÙñÑ,;.:*\\-\\[\\]¿?¡!#@\\\"\\\"()_'/])\",' ',pre) #Eliminar caracteres especiales como emoticones, exceptuando los signos de puntuación y tildes.\n",
    "    pre = re.sub(r'([;.:\\-\\[\\]¿?¡!#\\\"()]){3,}',r'\\1\\1',pre) #Si repite un caracters especial más de 3 veces ej. \"\"\"\"\n",
    "    pre = re.sub(r'([a-zA-Z])\\1{2,}',r'\\1\\1',pre) #Si repite una letra más de dos veces las reduce a dos repeticiones goool => gool    \n",
    "    #pre = re.sub(r'((?<=[A-Za-z])(?=[A-Z][a-zäÄëËïÏöÖüÜáéíóúáéíóúÁÉÍÓÚÂÊÎÔÛâêîôûàèìòùÀÈÌÒÙñÑ]))',r' ',pre) #Segmentados de palabras por mayuscula Ej. \"HolaMundo\" por \"Hola Mundo\"\n",
    "    pre = re.sub(r'(\\s){2,}',r' ',pre) #Eliminar espacios seguidos              \n",
    "    return pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    'En la Av Boyacá con 66 se presentó un atropellamiento de un peatón por una motocicleta; conductor de la motocicleta y peatón lesionados.#PROTEGERYSERVIR @puentearanda',\n",
    "    'Grave accidente de taxis congestionó el sector El Lago 🔴EN VIVO #CapitalNoticias https://t.co/sVikHeRcZA'\n",
    "]\n",
    "brief_cleaning = (preText(str(row)) for row in text)\n",
    "txt = [clean_fn(doc) for doc in nlp.pipe(brief_cleaning, batch_size=50, n_threads=4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['En la Av Boyacá con 66 se presentó un atropellamiento de un peatón por una motocicleta; conductor de la motocicleta y peatón lesionados.#PROTEGERYSERVIR puente aranda',\n",
       " 'Grave accidente de taxis congestionó el sector El Lago EN VIVO capital noticias']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generando archivos"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Barajando el dataset y seleccionando 50\n",
    "dataset = dataset.sample(frac=1)\n",
    "dataset.to_csv(\"../../../data/v1/NER/ner_dataset_shuffle.tsv\",sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generando txt y ann\n",
    "i=1\n",
    "for tweet in txt:\n",
    "    name = dir_+'brat/'+'tweet-'+str(i)+'.txt'\n",
    "    #print(name)  \n",
    "    with open(name, \"w\") as file:\n",
    "        file.write(tweet)\n",
    "    name = dir_+'brat/'+'tweet-'+str(i)+'.ann'\n",
    "    with open(name, \"w\") as file:\n",
    "        file.write('')\n",
    "    i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
