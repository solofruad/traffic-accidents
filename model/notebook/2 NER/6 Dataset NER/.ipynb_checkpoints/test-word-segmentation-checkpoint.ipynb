{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test # 1 Word Segmentation\n",
    "**Tomado de:** Norvig, P.: Natural language corpus data. In: Beautiful Data, pp. 219â€“242 (2009)\n",
    "\n",
    "URL\n",
    "* https://norvig.com/ngrams/\n",
    "* https://norvig.com/ngrams/ch14.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, random, glob, operator, heapq, functools\n",
    "from collections import defaultdict\n",
    "from math import log10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memo(f):\n",
    "    \"Memoize function f.\"\n",
    "    table = {}\n",
    "    def fmemo(*args):\n",
    "        if args not in table:\n",
    "            table[args] = f(*args)\n",
    "        return table[args]\n",
    "    fmemo.memo = table\n",
    "    return fmemo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memo\n",
    "def segment(text):\n",
    "    \"Return a list of words that is the best segmentation of text.\"\n",
    "    if not text: return []\n",
    "    candidates = ([first]+segment(rem) for first,rem in splits(text))    \n",
    "    return max(candidates, key=Pwords)\n",
    "\n",
    "def splits(text, L=20):\n",
    "    \"Return a list of all possible (first, rem) pairs, len(first)<=L.\"\n",
    "    return [(text[:i+1], text[i+1:]) \n",
    "            for i in range(min(len(text), L))]\n",
    "\n",
    "def Pwords(words): \n",
    "    \"The Naive Bayes probability of a sequence of words.\"\n",
    "    return product(Pw(w) for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Support functions (p. 224)\n",
    "\n",
    "def product(nums):\n",
    "    \"Return the product of a sequence of numbers.\"\n",
    "    return functools.reduce(operator.mul, nums, 1)\n",
    "\n",
    "class Pdist(dict):\n",
    "    \"A probability distribution estimated from counts in datafile.\"\n",
    "    def __init__(self, data=[], N=None, missingfn=None):\n",
    "        for key,count in data:\n",
    "            self[key] = self.get(key, 0) + int(count)\n",
    "        self.N = float(N or sum(self.itervalues()))\n",
    "        self.missingfn = missingfn or (lambda k, N: 1./N)\n",
    "    def __call__(self, key): \n",
    "        if key in self: return self[key]/self.N  \n",
    "        else: return self.missingfn(key, self.N)\n",
    "\n",
    "def datafile(name, sep='\\t'):\n",
    "    \"Read key,value pairs from file.\"\n",
    "    for line in open(name):\n",
    "        yield line.split(sep)\n",
    "\n",
    "def avoid_long_words(key, N):\n",
    "    \"Estimate the probability of an unknown word.\"\n",
    "    return 10./(N * 10**len(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1024908267229 ## Number of tokens\n",
    "Pw  = Pdist(datafile('count_1w.txt'), N, avoid_long_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['when',\n",
       " 'in',\n",
       " 'the',\n",
       " 'course',\n",
       " 'of',\n",
       " 'human',\n",
       " 'events',\n",
       " 'it',\n",
       " 'becomes',\n",
       " 'necessary']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment('wheninthecourseofhumaneventsitbecomesnecessary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'a',\n",
       " 'hole',\n",
       " 'in',\n",
       " 'the',\n",
       " 'ground',\n",
       " 'there',\n",
       " 'lived',\n",
       " 'a',\n",
       " 'hobbit',\n",
       " 'not',\n",
       " 'a',\n",
       " 'nasty',\n",
       " 'dirty',\n",
       " 'wet',\n",
       " 'hole',\n",
       " 'filled',\n",
       " 'with',\n",
       " 'the',\n",
       " 'ends',\n",
       " 'of',\n",
       " 'worms',\n",
       " 'and',\n",
       " 'an',\n",
       " 'oozy',\n",
       " 'smell',\n",
       " 'nor',\n",
       " 'yet',\n",
       " 'a',\n",
       " 'dry',\n",
       " 'bare',\n",
       " 'sandy',\n",
       " 'hole',\n",
       " 'with',\n",
       " 'nothing',\n",
       " 'in',\n",
       " 'it',\n",
       " 'to',\n",
       " 'sitdown',\n",
       " 'on',\n",
       " 'or',\n",
       " 'to',\n",
       " 'eat',\n",
       " 'it',\n",
       " 'was',\n",
       " 'a',\n",
       " 'hobbit',\n",
       " 'hole',\n",
       " 'and',\n",
       " 'that',\n",
       " 'means',\n",
       " 'comfort']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment('inaholeinthegroundtherelivedahobbitnotanastydirtywetholefilledwiththeendsofwormsandanoozysmellnoryetadrybaresandyholewithnothinginittositdownonortoeatitwasahobbitholeandthatmeanscomfort')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'otraperiodista'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.iloc[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../../')\n",
    "\n",
    "from classes.wordsegmentation import WordSegmentation\n",
    "\n",
    "dir_ = \"../../../data/v1/NER/\"\n",
    "file_segmentation = dir_+'spanish_count_1w_small_v2_twitter.txt'\n",
    "segmentation = WordSegmentation(file_segmentation)\n",
    "\n",
    "import pandas as pd\n",
    "samples = pd.read_csv('output-words-generator-v2.csv')\n",
    "\n",
    "result = []\n",
    "for i in range(len(samples[['original']])):    \n",
    "    pre = segmentation.segment(samples.iloc[i][1])\n",
    "    text = ' '.join(pre)\n",
    "    #result.append([samples.iloc[i][0],text])\n",
    "    result.append(text)\n",
    "\n",
    "samples['v3_twitter'] = result\n",
    "samples.to_csv('output-words-generator-v3-twitter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['por la via x la vida',\n",
       " 'otra periodista',\n",
       " 'gen mas energia',\n",
       " 'el divino nene',\n",
       " 'transito policia',\n",
       " 'cluster gastronomia',\n",
       " 'invias oficial',\n",
       " 'el mundo rueda x se',\n",
       " 'me paso a la fm',\n",
       " 'se humano',\n",
       " 'romero vive',\n",
       " 'vamos millos a la final',\n",
       " 'mejor periodo',\n",
       " 'conductor agresivo',\n",
       " 'caracolradio',\n",
       " 'transmiseria',\n",
       " 'no voy al andino',\n",
       " 'transi le ni o',\n",
       " 'el crimen del siglo',\n",
       " 'viajar y ganar',\n",
       " 'adoptada',\n",
       " 'feliz martes',\n",
       " 'chinche mejia',\n",
       " 'americas',\n",
       " 'mal parqueada',\n",
       " 'ayuno',\n",
       " 'se juega a esta hora',\n",
       " 'transito bogota d',\n",
       " 'los 120 segundos del gato',\n",
       " 'en desarrollo',\n",
       " 'jorge antonio vega',\n",
       " 'cronicas transmi',\n",
       " 'jota volatil',\n",
       " 'reportan',\n",
       " 'sitpbogota',\n",
       " 'temprano es mas bacano',\n",
       " 'noticiasrcn',\n",
       " 'canalrcn',\n",
       " 'rappi colombia',\n",
       " 'aun tengo hambre',\n",
       " 'cesar flechas',\n",
       " 'noctambulo city',\n",
       " 'el tiempo',\n",
       " 'otto gerardo',\n",
       " 'policiabogota',\n",
       " 'vivian salazar',\n",
       " 'peluches quique sam',\n",
       " 'fontibon',\n",
       " 'steven arce',\n",
       " 'la calera',\n",
       " 'tm ahora',\n",
       " 'bogota se mueve',\n",
       " 'noticias capital',\n",
       " 'prudencia en las calles',\n",
       " 'naela music',\n",
       " 'bluradioco',\n",
       " 'seguridadbog',\n",
       " 'la fm siempre',\n",
       " 'tranc',\n",
       " 'sangre',\n",
       " 'dictador 3',\n",
       " 'el ojo d la noche',\n",
       " 'el dandy',\n",
       " 'puente aranda',\n",
       " 'circunvalar',\n",
       " 'fontibon somos t',\n",
       " 'la farra de espumas',\n",
       " 'pacho herrera jr',\n",
       " 'domingo de memes',\n",
       " 'camarada motero',\n",
       " 'ma',\n",
       " 'civicos bog',\n",
       " 'movilidad',\n",
       " 'cava de rosas',\n",
       " 'boca de seleccion',\n",
       " 'ui municipalista',\n",
       " 'elsa ximena',\n",
       " 'animales bog',\n",
       " 'incidente vial',\n",
       " 'sitp ayuda',\n",
       " 'kindergarten classroom',\n",
       " 'central',\n",
       " 'el luquillo',\n",
       " 'entusiasmo',\n",
       " 'por los amigos tuiteros',\n",
       " 'la reina lagarto',\n",
       " 'rcn radio',\n",
       " 'sectormovilidad',\n",
       " 'imprudencia',\n",
       " 'noticiascaracol',\n",
       " 'el team de la furia',\n",
       " 'toyotas',\n",
       " 'noticias uno',\n",
       " 'arriba bogota',\n",
       " 'movilidad total',\n",
       " 'cablenoticias qr',\n",
       " 'la kalle',\n",
       " 'el espectador',\n",
       " 'ahora',\n",
       " 'seguimos con toda',\n",
       " 'atravez del vaso',\n",
       " 'bestias driving',\n",
       " 'transmilenio',\n",
       " 'caza infractor',\n",
       " 'avenida',\n",
       " 'pueblo',\n",
       " 'foro p',\n",
       " 'trampas mortales',\n",
       " 'cali calentura',\n",
       " 'urgente',\n",
       " 'impacto paranormal',\n",
       " 'reporte',\n",
       " 'socializar',\n",
       " 'atencion',\n",
       " 'facebook live',\n",
       " 'de espacio',\n",
       " 'medimas eps no se de tiene',\n",
       " 'bogota participa',\n",
       " 'fuera sanguinetti perdedor',\n",
       " 'bogota',\n",
       " 'tristeza',\n",
       " 'accidente de transito bogota',\n",
       " 'tranvia bgta',\n",
       " 'la luciernaga',\n",
       " 'red mas noticias',\n",
       " 'citynoticias fds',\n",
       " 'red mas tv',\n",
       " 'citynoticias md',\n",
       " 'bomberosbogota',\n",
       " 'am',\n",
       " 'info bogota et',\n",
       " 'noticia',\n",
       " 'normandia',\n",
       " 'tcc colombia',\n",
       " 'ciudad',\n",
       " 'alcaldia usaquen',\n",
       " 'bibliotecas p',\n",
       " 'ideal',\n",
       " 'red a p bogota',\n",
       " 'la reportera soy yo',\n",
       " 'comparte',\n",
       " 'a prevenir todos',\n",
       " 'pa trude la noche',\n",
       " 'registro ica',\n",
       " 'mama dos de petro',\n",
       " 'moto valle puente aranda',\n",
       " 'capital noticias',\n",
       " 'actualidad',\n",
       " 'en vivo',\n",
       " 'sectormovilidad 1',\n",
       " 'seguridad vial',\n",
       " 'mercedes benz world',\n",
       " 'accidente vial',\n",
       " 'noticia en desarrollo',\n",
       " 'que violencia',\n",
       " 'corferias bogota',\n",
       " 'youtube',\n",
       " 'bogo tiana',\n",
       " 'estamos trabajando',\n",
       " 'ultimahora',\n",
       " 'andenes',\n",
       " 'el tiempo tele',\n",
       " 'moto',\n",
       " 'arcadia',\n",
       " 'viernes',\n",
       " 'candelaria',\n",
       " 'canalcapital',\n",
       " 'expediente uribe v',\n",
       " 'transito bogota',\n",
       " 'pura pasion',\n",
       " 'gabo de las casas',\n",
       " 'bellavista',\n",
       " 'colombia hardcore',\n",
       " 'motero',\n",
       " 'world press photo',\n",
       " 'en este momento',\n",
       " 'fallon tonight',\n",
       " 'la mega',\n",
       " 'obteniendo conciencia',\n",
       " 'umv bogota',\n",
       " 'bogota abierta',\n",
       " '6am caracol',\n",
       " 'citynoticias',\n",
       " 'primer solo de fagot',\n",
       " 'atentos',\n",
       " 'lo c sancristobal',\n",
       " 'somos la 92',\n",
       " 'sabado santo',\n",
       " 'alcaldia kennedy',\n",
       " 'curules de las victimas https',\n",
       " 'la militar',\n",
       " 'travel awesome',\n",
       " 'mabel lara news',\n",
       " 'caracol tv',\n",
       " 'ley del embudo',\n",
       " 'red de apoyo si c',\n",
       " 'viviana rock',\n",
       " 'bogotatransito',\n",
       " 'maldita peli red',\n",
       " 'alkosto',\n",
       " 'amarilo construyendo pais',\n",
       " 'pentecostes version',\n",
       " 'parroquia la sagrada eucaristia',\n",
       " 'accidente',\n",
       " 'hospital suba',\n",
       " 'fiscalia col',\n",
       " 'desde el balc',\n",
       " 'idubogota',\n",
       " 'cesar sin arte',\n",
       " 'la fu cs sabe de salud',\n",
       " 'trasmilenio',\n",
       " 'colmo',\n",
       " 'chapinero',\n",
       " 'emergencias bgt',\n",
       " 'a esta hora',\n",
       " 'exceso de velocidad',\n",
       " 'fuera del estudio',\n",
       " 'reportando ando',\n",
       " 'concepcion tang',\n",
       " 'cruz roja bogota',\n",
       " 'alerta bogota',\n",
       " 'proteger y servir',\n",
       " 'que no te deje el a vi',\n",
       " 'ambiente bogota con',\n",
       " 'calle 80',\n",
       " 'transito',\n",
       " 'movilidad bogota',\n",
       " 'domingo fitness',\n",
       " 'policia',\n",
       " 'deporte y salud',\n",
       " 'feliz viernes',\n",
       " 'sector salud',\n",
       " 'avenida 68',\n",
       " 'trafico',\n",
       " 'tu transitobta',\n",
       " 'calera',\n",
       " 'rutas sitp',\n",
       " 'publimetro col',\n",
       " 'en desarr l lo',\n",
       " 'pendiente',\n",
       " 'patin carrera chile',\n",
       " 'vamos mi santafe',\n",
       " 'proteccion a lideres',\n",
       " 'el caracter de cristo',\n",
       " 'transitobta',\n",
       " 'bajemosle a la velocidad',\n",
       " 'artistas nacionales',\n",
       " 'muerto',\n",
       " 'todo se cura con amor',\n",
       " 'el noct',\n",
       " 'wradio colombia',\n",
       " 'hotel este que nda',\n",
       " 'ahuma mos lo que hacemos',\n",
       " 'localidad 16',\n",
       " 'calle 13',\n",
       " 'vi si oncerobog',\n",
       " 'pico y placa ambiental',\n",
       " 'colegio san bartolome la merced oficial',\n",
       " 'cablenoticias',\n",
       " 'chavismo energia y victoria',\n",
       " 'sitp',\n",
       " 'piel joven y tonificada',\n",
       " 'lineas de vida',\n",
       " 'la fm',\n",
       " 'cuarta fecha',\n",
       " 'te odio sol de bogot',\n",
       " 'amor felino',\n",
       " 'terapias mas al la',\n",
       " 'restrepo',\n",
       " 'policia colombia',\n",
       " 'vialidad',\n",
       " 'movilidad estereo',\n",
       " 'acueducto',\n",
       " 'sincandadoradio',\n",
       " 'semana santa',\n",
       " 'caracoles m',\n",
       " 'usme',\n",
       " 'maderas',\n",
       " 'toda mi boda',\n",
       " 'waze traffic bog',\n",
       " 'usaquen et',\n",
       " 'fracking colombia',\n",
       " 'alex en la calle',\n",
       " 'reporte ts',\n",
       " 'domicilios bogot',\n",
       " 'diario extra bogo',\n",
       " 'movistar co',\n",
       " 'al aire',\n",
       " 'objetado duque',\n",
       " 'deportes con',\n",
       " 'numeral 767',\n",
       " 'movilidad inteligente',\n",
       " 'cali',\n",
       " 'aguante la empanada',\n",
       " 'nqs']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#samples = pd.read_csv('samples_word_segmentation.csv')\n",
    "samples = pd.read_csv('output-words-generator-v2.csv')\n",
    "#samples[['original']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N = 2495613020 # spanish_count_1w_small\n",
    "#N = 2497358193 # spanish_count_1w_small_v2 sin acento\n",
    "N = 2575683488 # spanish_count_1w_small_v2 sin acento con Twitter\n",
    "Pw  = Pdist(datafile('spanish_count_1w_small_v2_twitter.txt'), N, avoid_long_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.int64' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a9f63556f7b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'original'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#result.append([samples.iloc[i][0],text])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-30ce091b5910>\u001b[0m in \u001b[0;36mfmemo\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfmemo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mfmemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-9cb8f6463c7c>\u001b[0m in \u001b[0;36msegment\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"Return a list of words that is the best segmentation of text.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-9cb8f6463c7c>\u001b[0m in \u001b[0;36msplits\u001b[0;34m(text, L)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;34m\"Return a list of all possible (first, rem) pairs, len(first)<=L.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     return [(text[:i+1], text[i+1:]) \n\u001b[0;32m---> 11\u001b[0;31m             for i in range(min(len(text), L))]\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mPwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.int64' has no len()"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for i in range(len(samples[['original']])):\n",
    "    pre = segment(samples.iloc[i][0])\n",
    "    text = ' '.join(pre)\n",
    "    #result.append([samples.iloc[i][0],text])\n",
    "    result.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['por la via x la vida',\n",
       " 'otra periodista',\n",
       " 'gen mas energia',\n",
       " 'el divino nene',\n",
       " 'transito policia',\n",
       " 'cluster gastronomia',\n",
       " 'invias oficial',\n",
       " 'el mundo rueda x se',\n",
       " 'me paso a la fm',\n",
       " 'se humano',\n",
       " 'romero vive',\n",
       " 'vamos millos a la final',\n",
       " 'mejor periodo',\n",
       " 'conductor agresivo',\n",
       " 'caracolradio',\n",
       " 'transmiseria',\n",
       " 'no voy al andino',\n",
       " 'transi le ni o',\n",
       " 'el crimen del siglo',\n",
       " 'viajar y ganar',\n",
       " 'adoptada',\n",
       " 'feliz martes',\n",
       " 'chinche mejia',\n",
       " 'americas',\n",
       " 'mal parqueada',\n",
       " 'ayuno',\n",
       " 'se juega a esta hora',\n",
       " 'transito bogota d',\n",
       " 'los 120 segundos del gato',\n",
       " 'en desarrollo',\n",
       " 'jorge antonio vega',\n",
       " 'cronicas transmi',\n",
       " 'jota volatil',\n",
       " 'reportan',\n",
       " 'sitpbogota',\n",
       " 'temprano es mas bacano',\n",
       " 'noticiasrcn',\n",
       " 'canalrcn',\n",
       " 'rappi colombia',\n",
       " 'aun tengo hambre',\n",
       " 'cesar flechas',\n",
       " 'noctambulo city',\n",
       " 'el tiempo',\n",
       " 'otto gerardo',\n",
       " 'policiabogota',\n",
       " 'vivian salazar',\n",
       " 'peluches quique sam',\n",
       " 'fontibon',\n",
       " 'steven arce',\n",
       " 'la calera',\n",
       " 'tm ahora',\n",
       " 'bogota se mueve',\n",
       " 'noticias capital',\n",
       " 'prudencia en las calles',\n",
       " 'naela music',\n",
       " 'bluradioco',\n",
       " 'seguridadbog',\n",
       " 'la fm siempre',\n",
       " 'tranc',\n",
       " 'sangre',\n",
       " 'dictador 3',\n",
       " 'el ojo d la noche',\n",
       " 'el dandy',\n",
       " 'puente aranda',\n",
       " 'circunvalar',\n",
       " 'fontibon somos t',\n",
       " 'la farra de espumas',\n",
       " 'pacho herrera jr',\n",
       " 'domingo de memes',\n",
       " 'camarada motero',\n",
       " 'ma',\n",
       " 'civicos bog',\n",
       " 'movilidad',\n",
       " 'cava de rosas',\n",
       " 'boca de seleccion',\n",
       " 'ui municipalista',\n",
       " 'elsa ximena',\n",
       " 'animales bog',\n",
       " 'incidente vial',\n",
       " 'sitp ayuda',\n",
       " 'kindergarten classroom',\n",
       " 'central',\n",
       " 'el luquillo',\n",
       " 'entusiasmo',\n",
       " 'por los amigos tuiteros',\n",
       " 'la reina lagarto',\n",
       " 'rcn radio',\n",
       " 'sectormovilidad',\n",
       " 'imprudencia',\n",
       " 'noticiascaracol',\n",
       " 'el team de la furia',\n",
       " 'toyotas',\n",
       " 'noticias uno',\n",
       " 'arriba bogota',\n",
       " 'movilidad total',\n",
       " 'cablenoticias qr',\n",
       " 'la kalle',\n",
       " 'el espectador',\n",
       " 'ahora',\n",
       " 'seguimos con toda',\n",
       " 'atravez del vaso',\n",
       " 'bestias driving',\n",
       " 'transmilenio',\n",
       " 'caza infractor',\n",
       " 'avenida',\n",
       " 'pueblo',\n",
       " 'foro p',\n",
       " 'trampas mortales',\n",
       " 'cali calentura',\n",
       " 'urgente',\n",
       " 'impacto paranormal',\n",
       " 'reporte',\n",
       " 'socializar',\n",
       " 'atencion',\n",
       " 'facebook live',\n",
       " 'de espacio',\n",
       " 'medimas eps no se de tiene',\n",
       " 'bogota participa',\n",
       " 'fuera sanguinetti perdedor',\n",
       " 'bogota',\n",
       " 'tristeza',\n",
       " 'accidente de transito bogota',\n",
       " 'tranvia bgta',\n",
       " 'la luciernaga',\n",
       " 'red mas noticias',\n",
       " 'citynoticias fds',\n",
       " 'red mas tv',\n",
       " 'citynoticias md',\n",
       " 'bomberosbogota',\n",
       " 'am',\n",
       " 'info bogota et',\n",
       " 'noticia',\n",
       " 'normandia',\n",
       " 'tcc colombia',\n",
       " 'ciudad',\n",
       " 'alcaldia usaquen',\n",
       " 'bibliotecas p',\n",
       " 'ideal',\n",
       " 'red a p bogota',\n",
       " 'la reportera soy yo',\n",
       " 'comparte',\n",
       " 'a prevenir todos',\n",
       " 'pa trude la noche',\n",
       " 'registro ica',\n",
       " 'mama dos de petro',\n",
       " 'moto valle puente aranda',\n",
       " 'capital noticias',\n",
       " 'actualidad',\n",
       " 'en vivo',\n",
       " 'sectormovilidad 1',\n",
       " 'seguridad vial',\n",
       " 'mercedes benz world',\n",
       " 'accidente vial',\n",
       " 'noticia en desarrollo',\n",
       " 'que violencia',\n",
       " 'corferias bogota',\n",
       " 'youtube',\n",
       " 'bogo tiana',\n",
       " 'estamos trabajando',\n",
       " 'ultimahora',\n",
       " 'andenes',\n",
       " 'el tiempo tele',\n",
       " 'moto',\n",
       " 'arcadia',\n",
       " 'viernes',\n",
       " 'candelaria',\n",
       " 'canalcapital',\n",
       " 'expediente uribe v',\n",
       " 'transito bogota',\n",
       " 'pura pasion',\n",
       " 'gabo de las casas',\n",
       " 'bellavista',\n",
       " 'colombia hardcore',\n",
       " 'motero',\n",
       " 'world press photo',\n",
       " 'en este momento',\n",
       " 'fallon tonight',\n",
       " 'la mega',\n",
       " 'obteniendo conciencia',\n",
       " 'umv bogota',\n",
       " 'bogota abierta',\n",
       " '6am caracol',\n",
       " 'citynoticias',\n",
       " 'primer solo de fagot',\n",
       " 'atentos',\n",
       " 'lo c sancristobal',\n",
       " 'somos la 92',\n",
       " 'sabado santo',\n",
       " 'alcaldia kennedy',\n",
       " 'curules de las victimas https',\n",
       " 'la militar',\n",
       " 'travel awesome',\n",
       " 'mabel lara news',\n",
       " 'caracol tv',\n",
       " 'ley del embudo',\n",
       " 'red de apoyo si c',\n",
       " 'viviana rock',\n",
       " 'bogotatransito',\n",
       " 'maldita peli red',\n",
       " 'alkosto',\n",
       " 'amarilo construyendo pais',\n",
       " 'pentecostes version',\n",
       " 'parroquia la sagrada eucaristia',\n",
       " 'accidente',\n",
       " 'hospital suba',\n",
       " 'fiscalia col',\n",
       " 'desde el balc',\n",
       " 'idubogota',\n",
       " 'cesar sin arte',\n",
       " 'la fu cs sabe de salud',\n",
       " 'trasmilenio',\n",
       " 'colmo',\n",
       " 'chapinero',\n",
       " 'emergencias bgt',\n",
       " 'a esta hora',\n",
       " 'exceso de velocidad',\n",
       " 'fuera del estudio',\n",
       " 'reportando ando',\n",
       " 'concepcion tang',\n",
       " 'cruz roja bogota',\n",
       " 'alerta bogota',\n",
       " 'proteger y servir',\n",
       " 'que no te deje el a vi',\n",
       " 'ambiente bogota con',\n",
       " 'calle 80',\n",
       " 'transito',\n",
       " 'movilidad bogota',\n",
       " 'domingo fitness',\n",
       " 'policia',\n",
       " 'deporte y salud',\n",
       " 'feliz viernes',\n",
       " 'sector salud',\n",
       " 'avenida 68',\n",
       " 'trafico',\n",
       " 'tu transitobta',\n",
       " 'calera',\n",
       " 'rutas sitp',\n",
       " 'publimetro col',\n",
       " 'en desarr l lo',\n",
       " 'pendiente',\n",
       " 'patin carrera chile',\n",
       " 'vamos mi santafe',\n",
       " 'proteccion a lideres',\n",
       " 'el caracter de cristo',\n",
       " 'transitobta',\n",
       " 'bajemosle a la velocidad',\n",
       " 'artistas nacionales',\n",
       " 'muerto',\n",
       " 'todo se cura con amor',\n",
       " 'el noct',\n",
       " 'wradio colombia',\n",
       " 'hotel este que nda',\n",
       " 'ahuma mos lo que hacemos',\n",
       " 'localidad 16',\n",
       " 'calle 13',\n",
       " 'vi si oncerobog',\n",
       " 'pico y placa ambiental',\n",
       " 'colegio san bartolome la merced oficial',\n",
       " 'cablenoticias',\n",
       " 'chavismo energia y victoria',\n",
       " 'sitp',\n",
       " 'piel joven y tonificada',\n",
       " 'lineas de vida',\n",
       " 'la fm',\n",
       " 'cuarta fecha',\n",
       " 'te odio sol de bogot',\n",
       " 'amor felino',\n",
       " 'terapias mas al la',\n",
       " 'restrepo',\n",
       " 'policia colombia',\n",
       " 'vialidad',\n",
       " 'movilidad estereo',\n",
       " 'acueducto',\n",
       " 'sincandadoradio',\n",
       " 'semana santa',\n",
       " 'caracoles m',\n",
       " 'usme',\n",
       " 'maderas',\n",
       " 'toda mi boda',\n",
       " 'waze traffic bog',\n",
       " 'usaquen et',\n",
       " 'fracking colombia',\n",
       " 'alex en la calle',\n",
       " 'reporte ts',\n",
       " 'domicilios bogot',\n",
       " 'diario extra bogo',\n",
       " 'movistar co',\n",
       " 'al aire',\n",
       " 'objetado duque',\n",
       " 'deportes con',\n",
       " 'numeral 767',\n",
       " 'movilidad inteligente',\n",
       " 'cali',\n",
       " 'aguante la empanada',\n",
       " 'nqs']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples['v2_twitter'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>word_segmentation</th>\n",
       "      <th>Correct</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>porlaviaxlavida</td>\n",
       "      <td>por la via x la vida</td>\n",
       "      <td>1</td>\n",
       "      <td>por la via x la vida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>otraperiodista</td>\n",
       "      <td>otra periodista</td>\n",
       "      <td>1</td>\n",
       "      <td>otra periodista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>genmasenergia</td>\n",
       "      <td>gen mas energia</td>\n",
       "      <td>1</td>\n",
       "      <td>gen mas energia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eldivinonene</td>\n",
       "      <td>el divino nene</td>\n",
       "      <td>1</td>\n",
       "      <td>el divino nene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>transitopolicia</td>\n",
       "      <td>transito policia</td>\n",
       "      <td>1</td>\n",
       "      <td>transito policia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>numeral767</td>\n",
       "      <td>numeral 767</td>\n",
       "      <td>1</td>\n",
       "      <td>numeral 767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>movilidadinteligente</td>\n",
       "      <td>movilidad inteligente</td>\n",
       "      <td>1</td>\n",
       "      <td>movilidad inteligente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>cali</td>\n",
       "      <td>cali</td>\n",
       "      <td>1</td>\n",
       "      <td>cali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>aguantelaempanada</td>\n",
       "      <td>aguante la empanada</td>\n",
       "      <td>1</td>\n",
       "      <td>aguante la empanada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>nqs</td>\n",
       "      <td>nqs</td>\n",
       "      <td>1</td>\n",
       "      <td>nqs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>295 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 original      word_segmentation  Correct  \\\n",
       "0         porlaviaxlavida   por la via x la vida        1   \n",
       "1          otraperiodista        otra periodista        1   \n",
       "2           genmasenergia        gen mas energia        1   \n",
       "3            eldivinonene         el divino nene        1   \n",
       "4         transitopolicia       transito policia        1   \n",
       "..                    ...                    ...      ...   \n",
       "290            numeral767            numeral 767        1   \n",
       "291  movilidadinteligente  movilidad inteligente        1   \n",
       "292                  cali                   cali        1   \n",
       "293     aguantelaempanada    aguante la empanada        1   \n",
       "294                   nqs                    nqs        1   \n",
       "\n",
       "                        v2  \n",
       "0     por la via x la vida  \n",
       "1          otra periodista  \n",
       "2          gen mas energia  \n",
       "3           el divino nene  \n",
       "4         transito policia  \n",
       "..                     ...  \n",
       "290            numeral 767  \n",
       "291  movilidad inteligente  \n",
       "292                   cali  \n",
       "293    aguante la empanada  \n",
       "294                    nqs  \n",
       "\n",
       "[295 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.to_csv('output-words-generator-v2-twitter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(result,columns=['original','word_segmentation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>word_segmentation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>porlaviaxlavida</td>\n",
       "      <td>por la via x la vida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>otraperiodista</td>\n",
       "      <td>otra periodista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>genmasenergia</td>\n",
       "      <td>gen mas energia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eldivinonene</td>\n",
       "      <td>el divino nene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>claudia14175900</td>\n",
       "      <td>claudia 14175900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>aetapi</td>\n",
       "      <td>a etap i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>fontib</td>\n",
       "      <td>fonti b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>aguantelaempanada</td>\n",
       "      <td>aguante la empanada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>nqs</td>\n",
       "      <td>nqs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>713 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              original     word_segmentation\n",
       "0      porlaviaxlavida  por la via x la vida\n",
       "1       otraperiodista       otra periodista\n",
       "2        genmasenergia       gen mas energia\n",
       "3         eldivinonene        el divino nene\n",
       "4      claudia14175900      claudia 14175900\n",
       "..                 ...                   ...\n",
       "708             aetapi              a etap i\n",
       "709             fontib               fonti b\n",
       "710                  d                     d\n",
       "711  aguantelaempanada   aguante la empanada\n",
       "712                nqs                   nqs\n",
       "\n",
       "[713 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_csv('words-generator.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N = 1911392132\n",
    "#Pw  = Pdist(datafile('datos-colombia.txt'), N, avoid_long_words)\n",
    "#N = 2495613020 # spanish_count_1w_small\n",
    "N = 2497358193 # spanish_count_1w_small_v3 sin acento\n",
    "Pw  = Pdist(datafile('spanish_count_1w_small_v2.txt'), N, avoid_long_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accion', 'poetica']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment('accionpoetica')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd Version\n",
    "Bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cPw(word, prev):\n",
    "    \"Conditional probability of word, given previous word.\"\n",
    "    try:\n",
    "        return P2w[prev + ' ' + word]/float(Pw[prev])\n",
    "    except KeyError:\n",
    "        return Pw(word)\n",
    "\n",
    "@memo \n",
    "def segment2(text, prev='<S>'): \n",
    "    \"Return (log P(words), words), where words is the best segmentation.\" \n",
    "    if not text: return 0.0, [] \n",
    "    candidates = [combine(log10(cPw(first, prev)), first, *segment2(rem, first)) \n",
    "                  for first,rem in splits(text)] \n",
    "    return max(candidates) \n",
    "\n",
    "def combine(Pfirst, first, Prem, rem): \n",
    "    \"Combine first and rem results into one (probability, words) pair.\" \n",
    "    return Pfirst+Prem, [first]+rem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "P2w = Pdist(datafile('count_2w.txt'), N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-164.42574174168834,\n",
       " ['in',\n",
       "  'a',\n",
       "  'hole',\n",
       "  'in',\n",
       "  'the',\n",
       "  'ground',\n",
       "  'there',\n",
       "  'lived',\n",
       "  'a',\n",
       "  'hobbit',\n",
       "  'not',\n",
       "  'a',\n",
       "  'nasty',\n",
       "  'dirty',\n",
       "  'wet',\n",
       "  'hole',\n",
       "  'filled',\n",
       "  'with',\n",
       "  'the',\n",
       "  'ends',\n",
       "  'of',\n",
       "  'worms',\n",
       "  'and',\n",
       "  'an',\n",
       "  'oozy',\n",
       "  'smell',\n",
       "  'nor',\n",
       "  'yet',\n",
       "  'a',\n",
       "  'dry',\n",
       "  'bare',\n",
       "  'sandy',\n",
       "  'hole',\n",
       "  'with',\n",
       "  'nothing',\n",
       "  'in',\n",
       "  'it',\n",
       "  'to',\n",
       "  'sit',\n",
       "  'down',\n",
       "  'on',\n",
       "  'or',\n",
       "  'to',\n",
       "  'eat',\n",
       "  'it',\n",
       "  'was',\n",
       "  'a',\n",
       "  'hobbit',\n",
       "  'hole',\n",
       "  'and',\n",
       "  'that',\n",
       "  'means',\n",
       "  'comfort'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment2('inaholeinthegroundtherelivedahobbitnotanastydirtywetholefilledwiththeendsofwormsandanoozysmellnoryetadrybaresandyholewithnothinginittositdownonortoeatitwasahobbitholeandthatmeanscomfort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-76.45118719416872,\n",
       " ['we',\n",
       "  'could',\n",
       "  'incorporate',\n",
       "  'more',\n",
       "  'data',\n",
       "  'and',\n",
       "  'either',\n",
       "  'keep',\n",
       "  'more',\n",
       "  'entries',\n",
       "  'from',\n",
       "  'the',\n",
       "  'unigram',\n",
       "  'or',\n",
       "  'bigram',\n",
       "  'data',\n",
       "  'or',\n",
       "  'perhaps',\n",
       "  'add',\n",
       "  'trigram',\n",
       "  'data'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment2('wecouldincorporatemoredataandeitherkeepmoreentriesfromtheunigramorbigramdataorperhapsaddtrigramdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-12.205356599285366, ['wonder', 'worman'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment2('wonderworman')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
